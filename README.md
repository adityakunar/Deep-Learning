# DeepLearning
We studied the effects of dropout as a regularizer to prevent overfitting for deep neural networks. Our main aim was to reproduce the findings of the original paper. We conducted experiments with different flavours of dropout i.e Bernoulli and Gaussian on multiple datasets such as MNIST and CIFAR10 and analyzed our results with respect to the paper. Please find our blogpost detailing our findings [here](DeepLearningReport.ipynb).

All notebooks can be found [here](code/)

### Team members
* Andrea Alfieri ([@andreaalf97](http://github.com/andreaalf97))
* Sharon Grundmann ([@shayorshay](http://github.com/shayorshay))
* Aditya Kunar ([@adityakunar](http://github.com/adityakunar))
* Avinash Saravanan ([@asarav](http://github.com/asarav))

### Useful Links
* [Paper](http://jmlr.org/papers/v15/srivastava14a.html)
* [Google Colab for Final Report](https://colab.research.google.com/drive/17n26ndsDuTVR0DV7oDUxpCls-qik-P6W)
